{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This noteboook explores how to provide a composite score from unsupervised models that output a percentile rank and confidence, and where a compositor object can ascribe importances to each model.\n",
    "\n",
    "# Example model outputs for a score value and confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from ipywidgets import FloatSlider, GridspecLayout, HTML\n",
    "import numpy as np\n",
    "\n",
    "def calculate_score(models):\n",
    "    \n",
    "    # We combine the confidence and importance from a model into one score\n",
    "    for model in models:\n",
    "        model['confidence_importance'] = np.sqrt(model['confidence'] * model['importance'])\n",
    "        \n",
    "    # Then normalize by that combined confidence and importance score\n",
    "    total = 0\n",
    "    for model in models:\n",
    "        total += model['confidence_importance']\n",
    "        \n",
    "    for model in models:\n",
    "        model['relative_weight'] = model['confidence_importance'] / total\n",
    "        model['contribution'] = model['relative_weight'] * model['value']\n",
    "        \n",
    "    composite_score = sum(model['contribution'] for model in models)\n",
    "    \n",
    "    return composite_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputs from individual models\n",
    "\n",
    "Each model outputs:\n",
    "\n",
    "  * A 0-1 quantile value\n",
    "  * A (pseudo-)confidence score.\n",
    "  \n",
    "The umbrella model imposes:\n",
    "\n",
    "  * An importance score.\n",
    "  \n",
    "The confidence and importance values for each model get conflated via:\n",
    "\n",
    "  $x_{i} = \\sqrt{confidence_{i} \\cdot importance_{i}}$\n",
    "  \n",
    "Then we sum these values so that we can normalize.\n",
    "\n",
    "  $X = \\Sigma_{i} x_{i}$\n",
    "  \n",
    "Then, we normalize the conflated term to get a relative weight for the model:\n",
    "\n",
    "  $w_{i} = x_{i} / X$\n",
    "  \n",
    "And set the contribution to:\n",
    "\n",
    "  $m_{i} = w_{i} \\cdot quantile\\_score_{i}$\n",
    "  \n",
    "And get the composite score:\n",
    "\n",
    "  $\\Sigma_{i} m_{i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'value': 0.5, 'confidence': 1.0, 'importance': 0.3333333333333333},\n",
       " {'value': 0.5, 'confidence': 0.01, 'importance': 0.3333333333333333},\n",
       " {'value': 0.5, 'confidence': 0.5, 'importance': 0.3333333333333333}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = []\n",
    "models.append(dict(value=.5, confidence=1.))\n",
    "models.append(dict(value=.5, confidence=.01))\n",
    "models.append(dict(value=.5, confidence=.5))\n",
    "for model in models:\n",
    "    model['importance'] = 1/len(models)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_score(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd3b8c8e1f341fe869f99a26828437c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(HTML(value='<h4>Value</h4>', layout=Layout(grid_area='widget001')), FloatSlider(valueâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite score: 0.500\r"
     ]
    }
   ],
   "source": [
    "n_models = len(models)\n",
    "value_sliders = [FloatSlider(value=m['value'], min=.0, max=1., step=.01) for m in models]\n",
    "confidence_sliders = [FloatSlider(value=m['confidence'], min=.0, max=1., step=.01) for m in models]\n",
    "importance_sliders = [FloatSlider(value=m['importance'], min=0.01, max=1., step=.01) for m in models]\n",
    "    \n",
    "def handle_value_slider_change(change):\n",
    "    global value_sliders, models\n",
    "    for slider, model in zip(value_sliders, models):\n",
    "        if change['owner'] == slider:\n",
    "            model['value'] = change['new']\n",
    "            break\n",
    "    sys.stdout.write(f'Composite score: {calculate_score(models):.3f}\\r')\n",
    "    \n",
    "def handle_confidence_slider_change(change):\n",
    "    global confidence_sliders, models\n",
    "    for slider, model in zip(confidence_sliders, models):\n",
    "        if change['owner'] == slider:\n",
    "            model['confidence'] = change['new']\n",
    "            break\n",
    "    sys.stdout.write(f'Composite score: {calculate_score(models):.3f}\\r')\n",
    "\n",
    "def handle_importance_slider_change(change):\n",
    "    global importance_sliders, models\n",
    "    \n",
    "    total = 0\n",
    "    for slider in importance_sliders:\n",
    "        if slider != change['owner']:\n",
    "            total += slider.value\n",
    "        slider.unobserve(handle_importance_slider_change, names='value')\n",
    "    \n",
    "    if total == 0:\n",
    "        total = .01\n",
    "    for slider in importance_sliders:\n",
    "        if slider != change['owner']:\n",
    "            slider.value *= (1 - change['new']) / total\n",
    "            \n",
    "    for model, slider in zip(models, importance_sliders):\n",
    "        slider.observe(handle_importance_slider_change, names='value')\n",
    "        model['importance'] = slider.value\n",
    "    \n",
    "    sys.stdout.write(f'Composite score: {calculate_score(models):.3f}\\r')\n",
    "    \n",
    "for slider in value_sliders:\n",
    "    slider.observe(handle_value_slider_change, names='value')\n",
    "for slider in confidence_sliders:\n",
    "    slider.observe(handle_confidence_slider_change, names='value')\n",
    "for slider in importance_sliders:\n",
    "    slider.observe(handle_importance_slider_change, names='value')\n",
    "\n",
    "grid = GridspecLayout(n_models+1, 3)\n",
    "\n",
    "header = HTML(\"<h4>Value</h4>\")\n",
    "grid[0, 0] = header\n",
    "for i, slider in enumerate(value_sliders):\n",
    "    grid[i+1, 0] = slider\n",
    "    \n",
    "header = HTML(\"<h4>Confidence</h4>\")\n",
    "grid[0, 1] = header\n",
    "for i, slider in enumerate(confidence_sliders):\n",
    "    grid[i+1, 1] = slider\n",
    "\n",
    "header = HTML(\"<h4>Importance (sum to 1)</h4>\")\n",
    "grid[0, 2] = header\n",
    "for i, slider in enumerate(importance_sliders):\n",
    "    grid[i+1, 2] = slider\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
